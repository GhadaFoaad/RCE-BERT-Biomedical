{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qD0ZDYhAD5LL"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas\n",
        "#!pip install gensim\n",
        "#!pip install pandas\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel, get_scheduler\n",
        "#from transformers import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import ast\n",
        "from collections import defaultdict\n",
        "#from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import datetime\n",
        "!pip install transformers datasets accelerate\n",
        "\n",
        "# Define parameters\n",
        "EMBEDDING_DIM = 32  # Dimension for Word2Vec embeddings\n",
        "MAX_SEQ_LENGTH = 256  # Max length for tokenized text\n",
        "BATCH_SIZE = 16 # Batch size for training\n",
        "#CSV_FILE_PATH = \"MeDaL_with_semantic_types_and_Relations.csv\"  # Path to your CSV file\n",
        "\n",
        "CSV_FILE_PATH = \"./data/data.csv\"\n",
        "\n",
        "#Replace with generated file names from preprocessing\n",
        "SEMANTIC_EMBEDDINGS_PATH = \"data_semantic_type_embeddings.txt\" # Path to save semantic type embeddings\n",
        "SEMANTIC_RELATIONS_PATH = \"data_semantic_relation_embeddings_last.txt\" # Path to save semantic relation embeddings\n",
        "VOCAB_SEMANTIC_TYPES_PATH = \"data_semantic_type_vocab_last.txt\" #Path to save vocabulary for semantic types\n",
        "VOCAB_RELATIONS_PATH = \"data_semantic_relation_vocab_last.txt\" #Path to save vocabulary for relations\n",
        "\n",
        "OPTIMIZER_TYPE = \"AdamW\"  # Choose \"Adam\", \"SGD\", or \"AdamW\"\n",
        "LEARNING_RATE = 2.2e-5\n",
        "EPOCHS = 7  # Number of training epochs\n",
        "embeddings_2 = {}\n",
        "\n",
        "# Define UMLS API details\n",
        "UMLS_API_KEY = \"\" #replace with your key\n",
        "UMLS_API_URL = \"https://uts-ws.nlm.nih.gov/rest\"\n",
        "\n",
        "#CSV_FILE_PATH = \"MSH_All_dataset_with_GoldRelations_withcontext_last.csv\"  # Path to your CSV file\n",
        "\n",
        "\n",
        "# Change Model name here\n",
        "MODEL_NAME = \"GanjinZero/UMLSBert_ENG\"\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv(CSV_FILE_PATH)\n",
        "print(\"Data loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8xT54kMteVn"
      },
      "source": [
        "# **Full Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4cipkNgqz1u"
      },
      "outputs": [],
      "source": [
        "\n",
        "BATCH_SIZE = 16  # Reduce the batch size\n",
        "\n",
        "MAX_SEQ_LENGTH = 256  # Reduce the sequence length if possible\n",
        "\n",
        "EMBEDDING_DIM = 32  # BioBERT's hidden dimension\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv(CSV_FILE_PATH)\n",
        "\n",
        "print(\"Data Loaded\")\n",
        "print(\"length of data before augmentation is:\", len(df))\n",
        "\n",
        "\n",
        "print(\"----Run Info.----\")\n",
        "num_stypes =df['GOLD_SEMANTIC_ENCODING'].nunique() +1 # Get the number of unique labels from your data.\n",
        "num_labels = num_stypes\n",
        "print(f\"No. of unique labels from data: {num_stypes}\")\n",
        "print(f\"No. of unique labels assigned to classifier: {num_labels}\")\n",
        "\n",
        "import datetime\n",
        "now =datetime.datetime.now()\n",
        "print(\"Current execusion time:\", now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "print(\"---------\")\n",
        "\n",
        "# Load semantic type and relation embeddings (from txt files)\n",
        "def load_embeddings(file_path, embedding_dim, vocabulary):\n",
        "    \"\"\"Loads word embeddings from a text file, handling errors for missing values or wrongly formatted rows.\"\"\"\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line_number, line in enumerate(f,1): # start index at 1\n",
        "            values = line.strip().split()\n",
        "            if len(values) > 1: #Check to make sure it has more than one element, as first element is the semantic type\n",
        "              word = values[0]\n",
        "              if word not in vocabulary: # only creates a vector if it is not present on vocabulary\n",
        "                try:\n",
        "                    vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float32)\n",
        "                    embeddings[word] = vector\n",
        "                except ValueError as e:\n",
        "                  #print(f\"Warning: Could not convert line to embeddings, ValueError {e} in line number {line_number} and content '{line}'\")\n",
        "                  embeddings[word] = torch.zeros(embedding_dim)  # Use a zero vector for unknown embeddings.\n",
        "              else:\n",
        "                  print(f\"Warning: word in vocab:Skipping line: {line}, as it's a semantic type in the vocab file. line number: {line_number}\")\n",
        "            else: #If not, then just skip the line\n",
        "                 print(f\"Warning: len(values)<1 in embedding file Skipping line: {line}. Line number: {line_number}\")\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "#load vocabularies\n",
        "def load_vocabularies(file_path):\n",
        "  \"\"\"loads vocabularies from file\"\"\"\n",
        "  vocabulary = set()\n",
        "  with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    for line_number, line in enumerate(f,1):\n",
        "       item = line.strip()\n",
        "       if item:\n",
        "          vocabulary.add(item)\n",
        "       else:\n",
        "          print(f\"Warning: Skipping empty line in vocabulary file '{file_path}'. Line Number is: {line_number}\")\n",
        "  return vocabulary\n",
        "\n",
        "#load embedding from file\n",
        "def load_embeddings_from_file(file_path, vocabulary):\n",
        "    \"\"\"Loads word embeddings from a text file, handling errors for missing values or wrongly formatted rows.\"\"\"\n",
        "    embeddings = {} # Initialize embeddings dictionary here\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line_number, line in enumerate(f,1):\n",
        "            if(line_number ==1):\n",
        "              continue\n",
        "            values = line.strip().split()\n",
        "            if len(values) > 1:\n",
        "                i = 1\n",
        "                while i < len(values) and values[i][0] not in {\"-\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"}:\n",
        "                    values[i]\n",
        "                    i += 1\n",
        "                word = \" \".join(values[:i])\n",
        "                try:\n",
        "                    vector = torch.tensor([float(val) for val in values[i:]], dtype=torch.float32)\n",
        "                    embeddings[word] = vector\n",
        "                except ValueError as e:\n",
        "                    embeddings[word] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "\n",
        "            else:\n",
        "                print(f\"Warning: len(values)<1 in embedding file Skipping line: {line}. Line number: {line_number}\")\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "# Load vocabularies\n",
        "semantic_types_vocabulary = load_vocabularies(VOCAB_SEMANTIC_TYPES_PATH)\n",
        "semantic_relations_vocabulary = load_vocabularies(VOCAB_RELATIONS_PATH)\n",
        "\n",
        "# Load embeddings\n",
        "semantic_type_embeddings = load_embeddings_from_file(SEMANTIC_EMBEDDINGS_PATH,semantic_types_vocabulary)\n",
        "semantic_relation_embeddings = load_embeddings_from_file(SEMANTIC_RELATIONS_PATH, semantic_relations_vocabulary)\n",
        "\n",
        "print(\"Semantic and Relation embeddings and vocabularies loaded!\")\n",
        "\n",
        "\n",
        "def set_seed(seed_value):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    # Essential for ensuring deterministic behavior\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "class BioWSDDataset(Dataset):\n",
        "        def __init__(self, df, tokenizer, max_length, semantic_type_embeddings, semantic_relation_embeddings, semantic_types_vocabulary, semantic_relations_vocabulary, augment_p = DATA_AUGMENTATION_PROB ):\n",
        "            self.df = df\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_length = max_length\n",
        "            self.semantic_type_embeddings = semantic_type_embeddings\n",
        "            self.semantic_relation_embeddings = semantic_relation_embeddings\n",
        "            self.semantic_types_vocabulary = semantic_types_vocabulary\n",
        "            self.semantic_relations_vocabulary = semantic_relations_vocabulary\n",
        "            self.augment_p = augment_p\n",
        "\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.df)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "             row = self.df.iloc[idx]\n",
        "\n",
        "            #Text Context embedding\n",
        "             text = row['TEXT']\n",
        "             #location = row['LOCATION']\n",
        "             #text = augment_text(text, location, p = self.augment_p)\n",
        "             concepts = row[\"concept_names\"]  # Get the abbreviation\n",
        "             encoded_text = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation =True, max_length = self.max_length) # Generate the input ids and the attention mask, using the current text\n",
        "             text_embeddings = encoded_text.input_ids.squeeze(0) # remove batch dimension\n",
        "             attention_mask = encoded_text.attention_mask.squeeze(0)\n",
        "             abbreviation = row['ABBREV'] # Get the abbreviation, needed for embedding creation\n",
        "\n",
        "             #Abbreviation Semantic Embeddings\n",
        "             abbrev_cui_semantic_types = row['abbrev_cui_semantic_types'] if pd.notna(row['abbrev_cui_semantic_types']) else \"[]\"\n",
        "             context_semantic_types = row['context_semantic_types'] if pd.notna(row['context_semantic_types']) else \"[]\"\n",
        "             abbrev_relations = row['abbrev_relations'] if pd.notna(row['abbrev_relations']) else \"[]\"\n",
        "             context_relations = row['context_relations'] if pd.notna(row['context_relations']) else \"[]\"\n",
        "\n",
        "             try:\n",
        "                  abbrev_cui_semantic_types = ast.literal_eval(abbrev_cui_semantic_types)\n",
        "                  context_semantic_types = ast.literal_eval(context_semantic_types)\n",
        "                  #print(abbrev_cui_semantic_types)\n",
        "                  abbrev_relations = ast.literal_eval(abbrev_relations)\n",
        "                  context_relations = ast.literal_eval(context_relations)\n",
        "             except (ValueError, TypeError) as e:\n",
        "                  print(f\"Skipping row {idx}. Invalid semantic/relation lists format or missing values. Error is: {e}\")\n",
        "                  return None\n",
        "\n",
        "             semantic_embeddings = self.create_semantic_embeddings(abbrev_cui_semantic_types,context_semantic_types, self.semantic_type_embeddings)\n",
        "             relation_embeddings = self.create_relation_embeddings(abbrev_relations,context_relations, self.semantic_relation_embeddings)\n",
        "\n",
        "\n",
        "             #Gold CUI Semantic Embeddings\n",
        "             gold_cui_semantic_types = row['GOLD_CUI_semantic_types'] if pd.notna(row['GOLD_CUI_semantic_types']) else \"[]\"\n",
        "             gold_cui_relations = row['gold_relations'] if pd.notna(row['gold_relations']) else \"[]\"\n",
        "             try:\n",
        "                   gold_cui_semantic_types = ast.literal_eval(gold_cui_semantic_types)\n",
        "                   gold_cui_relations = ast.literal_eval(gold_cui_relations)\n",
        "             except (ValueError, TypeError) as e:\n",
        "                    print(f\"Skipping row {idx}. Invalid semantic/relation lists format or missing values. Error is: {e}\")\n",
        "                    return None\n",
        "\n",
        "\n",
        "             gold_semantic_embeddings = self.create_semantic_embeddings(gold_cui_semantic_types, [], self.semantic_type_embeddings) # pass an empty list for the second CUI\n",
        "             gold_relation_embeddings = self.create_relation_embeddings(gold_cui_relations, [], self.semantic_relation_embeddings) # pass an empty list for the second CUI\n",
        "\n",
        "             # Label\n",
        "             label = torch.tensor(row['GOLD_SEMANTIC_ENCODING'], dtype=torch.long)\n",
        "             if label == -1: # Skip samples with incorrect label\n",
        "                return None\n",
        "             else:\n",
        "                 return text_embeddings, semantic_embeddings,relation_embeddings, gold_semantic_embeddings, gold_relation_embeddings, label,concepts, attention_mask, abbreviation #include the abbreviation\n",
        "\n",
        "        def create_semantic_embeddings(self, semantic_types1, semantic_types2, embeddings):\n",
        "            \"\"\"Creates a combined embedding for a given list of semantic types.\"\"\"\n",
        "            all_semantic_embeddings = []\n",
        "            if semantic_types1:\n",
        "                for stype in semantic_types1:\n",
        "                    if stype in embeddings:\n",
        "                        #print(f\"Semantic type is a string and found in embeddings: {stype}\")\n",
        "                        all_semantic_embeddings.append(embeddings[stype])\n",
        "                    else:\n",
        "                        print(f\"Warning: Semantic type for abbrev not found in embeddings: {stype}\")\n",
        "                        all_semantic_embeddings.append(torch.zeros(EMBEDDING_DIM)) # Use zero vector for unknown embeddings\n",
        "            else:\n",
        "                print(\"No semantic types for abbrev provided.\")\n",
        "                all_semantic_embeddings.append(torch.zeros(EMBEDDING_DIM)) # Use zero vector for unknown embeddings\n",
        "            if semantic_types2:\n",
        "              if isinstance(semantic_types2, str): # Check if it's a string, indicating possible errors in previous parsing\n",
        "                try:\n",
        "                    context_semantic_types = ast.literal_eval(semantic_types2)\n",
        "                    # Ensure it's a list of strings\n",
        "                    if isinstance(context_semantic_types, list) and all(isinstance(item, str) for item in context_semantic_types):\n",
        "                        for stype in context_semantic_types:\n",
        "                            if stype in embeddings:\n",
        "                                all_semantic_embeddings.append(embeddings[stype])\n",
        "                            else:\n",
        "                                print(f\"Warning: Semantic type for context not found in embeddings: {stype}\")\n",
        "                                all_semantic_embeddings.append(torch.zeros(EMBEDDING_DIM))\n",
        "                    else:\n",
        "                        # If it's not a list of strings, handle the unexpected format. Log the issue for debugging.\n",
        "                        print(f\"Unexpected format for context_semantic_types: {context_semantic_types}\")\n",
        "                        all_semantic_embeddings.append(torch.zeros(EMBEDDING_DIM))\n",
        "                except (SyntaxError, ValueError):\n",
        "                    # Handle parsing errors, log the issue for debugging.\n",
        "                    print(f\"Error parsing context_semantic_types: {semantic_types2}\")\n",
        "                    all_semantic_embeddings.append(torch.zeros(EMBEDDING_DIM))\n",
        "              elif isinstance(semantic_types2, list):\n",
        "                #print(f\"Semantic types2: {semantic_types2}\")\n",
        "                # If it's already a list, iterate assuming items are strings\n",
        "                for stype in semantic_types2:\n",
        "                  if isinstance(stype, str) and stype in embeddings:  # Added isinstance check\n",
        "                     print(f\"Semantic type is a string and found in embeddings: {stype}\")\n",
        "                     all_semantic_embeddings.append(embeddings[stype])\n",
        "                  elif isinstance(stype, list):\n",
        "                     for item in stype:\n",
        "                        if item in embeddings:\n",
        "                           #print(f\"Semantic type is a list and found in embeddings: {item}\")\n",
        "                           all_semantic_embeddings.append(embeddings[item])\n",
        "                  else:\n",
        "                     print(f\"Warning: Semantic type for context not found in embeddings: {stype}\")\n",
        "                     all_semantic_embeddings.append(torch.zeros(EMBEDDING_DIM)) # Use zero vector for unknown embeddings\n",
        "\n",
        "            else:\n",
        "                #print(\"No semantic types for context provided.\")\n",
        "                all_semantic_embeddings.append(torch.zeros(EMBEDDING_DIM)) # Use zero vector for unknown embeddings\n",
        "\n",
        "            if all_semantic_embeddings:\n",
        "                return torch.mean(torch.stack(all_semantic_embeddings), dim=0) #Mean pooling to reduce to a single vector\n",
        "            else:\n",
        "                print(\"No valid semantic embeddings found.\")\n",
        "                return torch.zeros(EMBEDDING_DIM)  # zero vector if no embeddings\n",
        "\n",
        "        def create_relation_embeddings(self, relations1, relations2, embeddings):\n",
        "            \"\"\"Creates a combined embedding for a given list of relations.\"\"\"\n",
        "            all_relation_embeddings = []\n",
        "            if relations1:\n",
        "                for relation in relations1:  # Iterate directly through relations2\n",
        "                  # Ensure it's a list of strings\n",
        "                    if isinstance(relation, list):\n",
        "                        for rel in relation:\n",
        "                           try:\n",
        "                                r_label, r_concept = rel  # Attempt to unpack\n",
        "                                if (r_label, r_concept) in embeddings:\n",
        "                                    all_relation_embeddings.append(embeddings[(r_label, r_concept)])\n",
        "                                else:\n",
        "                                    all_relation_embeddings.append(torch.zeros(EMBEDDING_DIM))  # Use zero vector for unknown embeddings\n",
        "                           except ValueError:\n",
        "                                print(f\"Warning: Unexpected format in relations2: {rel}\")\n",
        "                    else:\n",
        "                          try:\n",
        "                              r_label, r_concept = relation  # Attempt to unpack\n",
        "                              if (r_label, r_concept) in embeddings:\n",
        "                                  all_relation_embeddings.append(embeddings[(r_label, r_concept)])\n",
        "                              else:\n",
        "                                  all_relation_embeddings.append(torch.zeros(EMBEDDING_DIM))  # Use zero vector for unknown embeddings\n",
        "                          except ValueError:\n",
        "                              print(f\"Warning: Unexpected format in relations1: {relation}\")  # Log unexpected formats\n",
        "\n",
        "            if relations2:\n",
        "                for relation in relations2:  # Iterate directly through relations2\n",
        "                  # Ensure it's a list of strings\n",
        "                    if isinstance(relation, list):\n",
        "                        for rel in relation:\n",
        "                           try:\n",
        "                                r_label, r_concept = rel  # Attempt to unpack\n",
        "                                if (r_label, r_concept) in embeddings:\n",
        "                                    all_relation_embeddings.append(embeddings[(r_label, r_concept)])\n",
        "                                else:\n",
        "                                    all_relation_embeddings.append(torch.zeros(EMBEDDING_DIM))  # Use zero vector for unknown embeddings\n",
        "                           except ValueError:\n",
        "                                print(f\"Warning: Unexpected format in relations2: {rel}\")\n",
        "                    else:\n",
        "                          try:\n",
        "                              r_label, r_concept = relation  # Attempt to unpack\n",
        "                              if (r_label, r_concept) in embeddings:\n",
        "                                  all_relation_embeddings.append(embeddings[(r_label, r_concept)])\n",
        "                              else:\n",
        "                                  all_relation_embeddings.append(torch.zeros(EMBEDDING_DIM))  # Use zero vector for unknown embeddings\n",
        "                          except ValueError:\n",
        "                              print(f\"Warning: Unexpected format in relations2: {relation}\")  # Log unexpected formats\n",
        "\n",
        "            if all_relation_embeddings:\n",
        "                return torch.mean(torch.stack(all_relation_embeddings), dim=0)  # Mean pooling to reduce to a single vector\n",
        "            else:\n",
        "                return torch.zeros(EMBEDDING_DIM)  # zero vector if no embeddings\n",
        "\n",
        "class BioWSDClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_labels, model_name = MODEL_NAME): # num_labels is now the number of unique CUI encodings\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.fc1 = nn.Linear(768*2 + 4 * embedding_dim + 768 , 256) #Combine text and both types of semantic embeddings\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(256, num_labels)\n",
        "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        self.to(self.device)\n",
        "\n",
        "\n",
        "    def forward(self, text_embeddings, semantic_embeddings, relation_embeddings, gold_semantic_embeddings, gold_relation_embeddings,concepts, label, attention_mask = None, abbreviation = None):\n",
        "        # Concatenate embeddings\n",
        "        text_embeddings = text_embeddings.to(self.device)\n",
        "        semantic_embeddings = semantic_embeddings.to(self.device)\n",
        "        relation_embeddings = relation_embeddings.to(self.device)\n",
        "        gold_semantic_embeddings = gold_semantic_embeddings.to(self.device)\n",
        "        gold_relation_embeddings = gold_relation_embeddings.to(self.device)\n",
        "\n",
        "        attention_mask = attention_mask.to(self.device)\n",
        "\n",
        "        bert_output = self.bert(input_ids = text_embeddings, attention_mask = attention_mask)\n",
        "        text_embeddings = bert_output.last_hidden_state[:, 0, :].to(self.device)  # Get embeddings of [CLS] token and move to device\n",
        "\n",
        "\n",
        "\n",
        "        # Move abbreviation tokenization and embedding generation to the correct device\n",
        "        abbreviation_input_ids = tokenizer(abbreviation, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(self.device)\n",
        "        abbreviation_embeddings = self.bert(input_ids=abbreviation_input_ids).last_hidden_state[:, 0, :].to(self.device)\n",
        "\n",
        "       if isinstance(abbreviation, tuple):\n",
        "          for abbr in abbreviation:\n",
        "            if abbr is None:\n",
        "              new_abbrev.append(f\"CUI: no concept\")\n",
        "            elif isinstance(abbr, str):\n",
        "              cui_specific =\"\"\n",
        "              for abbr_item in ast.literal_eval(abbr):\n",
        "                if abbr_item is None:\n",
        "                  new_abbrev.append(f\"CUI: no concept\")\n",
        "                  break\n",
        "                else:\n",
        "                  cui_specific += f\"CUI: {abbr_item} \"\n",
        "              if cui_specific != \"\":\n",
        "                new_abbrev.append(cui_specific) # Assuming you want the first element of the tuple\n",
        "                #print(\"cui specific\", cui_specific)\n",
        "            else:\n",
        "                   new_abbrev.append(f\"CUI: {abbr} \") # Assuming you want the first element of the tuple\n",
        "                   print(new_abbrev)\n",
        "\n",
        "        abbreviation =   \"[CLS] \" + abbreviation + \" [SEP]\",\n",
        "        concepts_input_ids = tokenizer(new_abbrev, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(self.device)\n",
        "        concepts_embeddings = self.bert(input_ids=abbreviation_input_ids).last_hidden_state[:, 0, :].to(self.device)\n",
        "\n",
        "        # Ensure concepts_embeddings and abbreviation_embeddings have the correct batch size\n",
        "        batch_size = text_embeddings.shape[0]  # Get the batch size\n",
        "        concepts_embeddings = concepts_embeddings[:batch_size]  # Truncate or pad if necessary\n",
        "        abbreviation = abbreviation[:batch_size]  # Truncate or pad if necessary\n",
        "\n",
        "\n",
        "        #print(\"concepts_embeddings shape:\", concepts_embeddings.shape)\n",
        "\n",
        "        combined_embeddings = torch.cat(\n",
        "            (text_embeddings, semantic_embeddings, relation_embeddings,gold_semantic_embeddings,gold_relation_embeddings, concepts_embeddings,abbreviation_embeddings), dim=1\n",
        "        ).to(self.device)\n",
        "\n",
        "        x = self.fc1(combined_embeddings)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        logits = self.fc2(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def measure_efficiency(model, dataloader, device):\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Measure Memory (Reset before starting)\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    start_time = time.time()\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            # Unpack the batch components based on BioWSDDataset.__getitem__\n",
        "            text_embeddings, semantic_embeddings, relation_embeddings, \\\n",
        "            gold_semantic_embeddings, gold_relation_embeddings, labels, \\\n",
        "            concepts, attention_mask, abbreviation = batch\n",
        "\n",
        "            # Move relevant inputs to device\n",
        "            text_embeddings = text_embeddings.to(device)\n",
        "            semantic_embeddings = semantic_embeddings.to(device)\n",
        "            relation_embeddings = relation_embeddings.to(device)\n",
        "            gold_semantic_embeddings = gold_semantic_embeddings.to(device)\n",
        "            gold_relation_embeddings = gold_relation_embeddings.to(device)\n",
        "            labels = labels.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            # Pass all required arguments to the model's forward method\n",
        "            outputs = model(\n",
        "                text_embeddings=text_embeddings,\n",
        "                semantic_embeddings=semantic_embeddings,\n",
        "                relation_embeddings=relation_embeddings,\n",
        "                gold_semantic_embeddings=gold_semantic_embeddings,\n",
        "                gold_relation_embeddings=gold_relation_embeddings,\n",
        "                concepts=concepts,\n",
        "                label=labels, # Pass label here even if not directly used for inference output\n",
        "                attention_mask=attention_mask,\n",
        "                abbreviation=abbreviation\n",
        "            )\n",
        "\n",
        "            total_samples += text_embeddings.size(0)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    # CALCULATIONS\n",
        "    latency_ms = (total_time * 1000) / total_samples\n",
        "    throughput = total_samples / total_time\n",
        "    # Get peak memory in GB\n",
        "    peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 3)\n",
        "\n",
        "    print(f\"--- Efficiency Metrics ---\")\n",
        "    print(f\"Total Samples: {total_samples}\")\n",
        "    print(f\"Total Time: {total_time:.4f}s\")\n",
        "    print(f\"Inference Latency: {latency_ms:.4f} ms/sample\")\n",
        "    print(f\"Throughput: {throughput:.2f} samples/sec\")\n",
        "    print(f\"Peak GPU Memory: {peak_memory:.4f} GB\")\n",
        "\n",
        "    return latency_ms, throughput, peak_memory\n",
        "\n",
        "#Set Seed\n",
        "seeds = [42, 43, 44] # try three different seeds\n",
        "all_results = []\n",
        "for seed in seeds:\n",
        "    print(f\"\\n=== Starting Experiment with Seed: {seed} ===\")\n",
        "    set_seed(seed)\n",
        "    SEED = seed\n",
        "\n",
        "    #1. Initialize Model & Dataloaders\n",
        "    model = BioWSDClassifier(EMBEDDING_DIM, num_labels)\n",
        "\n",
        "    # 1.1 Create dataset & DataLoader\n",
        "\n",
        "    # Split data to training and test\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
        "\n",
        "    # Create the datasets and data loaders\n",
        "    train_dataset = BioWSDDataset(train_df, tokenizer, MAX_SEQ_LENGTH, semantic_type_embeddings, semantic_relation_embeddings, semantic_types_vocabulary, semantic_relations_vocabulary)\n",
        "    test_dataset = BioWSDDataset(test_df,tokenizer, MAX_SEQ_LENGTH, semantic_type_embeddings, semantic_relation_embeddings, semantic_types_vocabulary, semantic_relations_vocabulary)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # 3. Optimizer and Scheduler\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    # Assuming you have 'train_dataloader' defined from your previous preprocessing steps:\n",
        "    num_training_steps = EPOCHS * len(train_dataloader) #You should use this in case you are running a train loop.\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "\n",
        "    print (\"Model Successfully Created\")\n",
        "    print (\"Data successfully prepared!\")\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    loss_fct = nn.CrossEntropyLoss() # Define loss function globally\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 2          # Stop if no improvement after 2 epochs\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"Starting training with Early Stopping (Patience={patience})...\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "\n",
        "        # --- TRAINING PHASE ---\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        # (Optional) Tqdm for training progress\n",
        "        progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            if batch is None: continue\n",
        "\n",
        "            # Unpack batch\n",
        "            text_embeddings, semantic_embeddings, relation_embeddings, gold_semantic_embeddings, gold_relation_embeddings, labels, concepts_embeddings, attention_mask, abbreviation = batch\n",
        "\n",
        "            # Move to device\n",
        "            text_embeddings = text_embeddings.to(device)\n",
        "            semantic_embeddings = semantic_embeddings.to(device)\n",
        "            relation_embeddings = relation_embeddings.to(device)\n",
        "            gold_semantic_embeddings = gold_semantic_embeddings.to(device)\n",
        "            gold_relation_embeddings = gold_relation_embeddings.to(device)\n",
        "            concepts_embeddings = concepts_embeddings.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            abbreviation = abbreviation # Strings don't need .to(device) usually, but your model handles it\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(text_embeddings, semantic_embeddings, relation_embeddings, gold_semantic_embeddings, gold_relation_embeddings, concepts_embeddings, labels, attention_mask=attention_mask, abbreviation=abbreviation)\n",
        "\n",
        "            loss = loss_fct(logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # --- VALIDATION PHASE (For Early Stopping) ---\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader: # Using test set as validation for efficiency\n",
        "                if batch is None: continue\n",
        "\n",
        "                text_embeddings, semantic_embeddings, relation_embeddings, gold_semantic_embeddings, gold_relation_embeddings, labels, concepts_embeddings, attention_mask, abbreviation = batch\n",
        "\n",
        "                # Move to device\n",
        "                text_embeddings = text_embeddings.to(device)\n",
        "                semantic_embeddings = semantic_embeddings.to(device)\n",
        "                relation_embeddings = relation_embeddings.to(device)\n",
        "                gold_semantic_embeddings = gold_semantic_embeddings.to(device)\n",
        "                gold_relation_embeddings = gold_relation_embeddings.to(device)\n",
        "                concepts_embeddings = concepts_embeddings.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                logits = model(text_embeddings, semantic_embeddings, relation_embeddings, gold_semantic_embeddings, gold_relation_embeddings, concepts_embeddings, labels, attention_mask=attention_mask, abbreviation=abbreviation)\n",
        "\n",
        "                loss = loss_fct(logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(test_dataloader)\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # --- EARLY STOPPING CHECK ---\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            # Save the best model state\n",
        "            torch.save(model.state_dict(), \"best_model_state.pt\")\n",
        "            print(\"Validation loss decreased. Saving model...\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"EarlyStopping counter: {patience_counter} out of {patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered! Stopping training.\")\n",
        "                break\n",
        "\n",
        "    # Load the best model before final evaluation\n",
        "    print(\"Loading best model for final evaluation...\")\n",
        "    model.load_state_dict(torch.load(\"best_model_state.pt\"))\n",
        "\n",
        "    print (\"Training Complete\")\n",
        "\n",
        "\n",
        "\n",
        "    # FINAL EVALUATION\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            if batch is None: continue\n",
        "            text_embeddings, semantic_embeddings, relation_embeddings, gold_semantic_embeddings, gold_relation_embeddings, labels, concepts_embeddings, attention_mask, abbreviation = batch\n",
        "\n",
        "            text_embeddings = text_embeddings.to(device)\n",
        "            semantic_embeddings = semantic_embeddings.to(device)\n",
        "            relation_embeddings = relation_embeddings.to(device)\n",
        "            gold_semantic_embeddings = gold_semantic_embeddings.to(device)\n",
        "            gold_relation_embeddings = gold_relation_embeddings.to(device)\n",
        "            concepts_embeddings = concepts_embeddings.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(text_embeddings, semantic_embeddings, relation_embeddings, gold_semantic_embeddings, gold_relation_embeddings, concepts_embeddings, labels, attention_mask=attention_mask, abbreviation=abbreviation)\n",
        "\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            true_labels.extend(labels.tolist())\n",
        "            predicted_labels.extend(predictions.tolist())\n",
        "\n",
        "    # --- Metrics Calculation ---\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "    macro_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
        "    weighted_f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "    weighted_precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "    weighted_recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "    print(\"--------------RESULTS-------------\")\n",
        "    print(f\"*****Model Name: {MODEL_NAME} | Seed: {SEED}*****\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  F1-score (Macro):  {macro_f1:.4f}\")\n",
        "    print(f\"  F1-score (Weighted):  {weighted_f1:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    results = {'seed': SEED, 'accuracy': accuracy, 'f1_macro': macro_f1, 'f1_weighted': weighted_f1}\n",
        "    all_results.append(results)\n",
        "    print(\"---------\")\n",
        "\n",
        "    # --- SPECIAL CHECKS (Efficiency & Error Analysis) ---\n",
        "    # Only run these on specific seeds to save time\n",
        "    if(SEED == 42): # Or whatever seed you prefer\n",
        "         measure_efficiency(model, test_dataloader, device)\n",
        "         print(\"---------\")\n",
        "    if(SEED == 44):\n",
        "         evaluate_and_save(model, test_dataloader, device, \"scibert_full_model_results.csv\")\n",
        "         print(\"---------\")\n",
        "\n",
        "# --- FINAL SUMMARY ---\n",
        "import statistics\n",
        "f1_weighted_scores = [res['f1_weighted'] for res in all_results]\n",
        "mean_score = statistics.mean(f1_weighted_scores)\n",
        "if len(f1_weighted_scores) > 1:\n",
        "    std_dev = statistics.stdev(f1_weighted_scores)\n",
        "else:\n",
        "    std_dev = 0.0\n",
        "\n",
        "print(f\"\\nFinal Result (Weighted F1): {mean_score:.4f} \\u00b1 {std_dev:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}