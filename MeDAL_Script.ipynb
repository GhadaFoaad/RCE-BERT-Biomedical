{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SCRIPT FOR CREATING A REPRODUCIBLE SUBSET ",
        "# ==============================================================================\n",
        "#\n",
        "# PURPOSE:\n",
        "# This script takes the full MeDAL corpus) and\n",
        "# creates a smaller, reproducible subset for experimentation. \n",
        "#  to ensure that results are robust\n",
        "# and that future research can use the exact same data partitions for fair\n",
        "# comparison.\n",
        "#\n",
        "# Author: G. M. Farouk\n",
        "# Date: 07/2025\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "\n",
        "\n",
       
        "N_SPLITS = 1\n",
        "\n",
       
        "# --- The column name in the CSV that will be CREATED to hold the unique ID for each row.\n",
        "INSTANCE_ID_COLUMN = 'instance_id'\n",
        "\n",
        "# --- The column name for your classification target.\n",
        "# This is used for stratified splitting to maintain class balance in each fold.\n",
        "# UPDATED to match dataset's \"LABEL\" column.\n",
        "TARGET_COLUMN = 'LABEL'\n"
      ],
      "metadata": {
        "id": "7DoqkgpgZ0bh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Paths to original dataset download\n",
        "!wget -nc -P data/ https://zenodo.org/record/4482922/files/train.csv\n",
        "!wget -nc -P data/ https://zenodo.org/record/4482922/files/valid.csv\n",
        "!wget -nc -P data/ https://zenodo.org/record/4482922/files/test.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvaxACz-Ge1x",
        "outputId": "20a765a3-cf66-45ec-9af7-714daa4ebd57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-28 18:25:24--  https://zenodo.org/record/4482922/files/train.csv\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.103.159, 188.185.79.172, 188.184.98.238, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.103.159|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/4482922/files/train.csv [following]\n",
            "--2024-09-28 18:25:24--  https://zenodo.org/records/4482922/files/train.csv\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3541556520 (3.3G) [text/plain]\n",
            "Saving to: ‘data/train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   3.30G  11.1MB/s    in 4m 27s  \n",
            "\n",
            "2024-09-28 18:29:52 (12.6 MB/s) - ‘data/train.csv’ saved [3541556520/3541556520]\n",
            "\n",
            "--2024-09-28 18:29:52--  https://zenodo.org/record/4482922/files/valid.csv\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.79.172, 188.184.98.238, 188.184.103.159, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.79.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/4482922/files/valid.csv [following]\n",
            "--2024-09-28 18:29:53--  https://zenodo.org/records/4482922/files/valid.csv\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1180795804 (1.1G) [text/plain]\n",
            "Saving to: ‘data/valid.csv’\n",
            "\n",
            "valid.csv           100%[===================>]   1.10G  17.8MB/s    in 59s     \n",
            "\n",
            "2024-09-28 18:30:52 (19.0 MB/s) - ‘data/valid.csv’ saved [1180795804/1180795804]\n",
            "\n",
            "--2024-09-28 18:30:53--  https://zenodo.org/record/4482922/files/test.csv\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.79.172, 188.184.98.238, 188.184.103.159, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.79.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/4482922/files/test.csv [following]\n",
            "--2024-09-28 18:30:53--  https://zenodo.org/records/4482922/files/test.csv\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1180152075 (1.1G) [text/plain]\n",
            "Saving to: ‘data/test.csv’\n",
            "\n",
            "test.csv            100%[===================>]   1.10G  12.7MB/s    in 1m 40s  \n",
            "\n",
            "2024-09-28 18:32:33 (11.3 MB/s) - ‘data/test.csv’ saved [1180152075/1180152075]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"medal\")"
      ],
      "metadata": {
        "id": "iyDE1xGAbDwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to local/drive storage of dataset (updated by user to his dataset downloaded location)\n",
        "DRIVE_PATH = '/content/drive/My Drive/'\n",
        "# Path where the output files will be saved\n",
        "OUTPUT_PATH = '/content/drive/My Drive/data_for_publication/'\n",
        "\n",
        "# Name of your original dataset file\n",
        "INPUT_FILENAME = 'medal_training.csv'\n",
        "\n",
        "# --- 2. SETUP AND DATA LOADING ---\n",
        "\n",
        "print(\"--- Starting Reproducible Subset Creation ---\")\n",
        "# Mount Google Drive to access files\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True) # force_remount can help avoid issues\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    # Exit if drive mounting fails\n",
        "    exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOANolV4b3n5",
        "outputId": "ae6f362c-c171-419a-df01-5aab20187ef6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Reproducible Subset Creation ---\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the output directory if it doesn't exist\n",
        "print(f\"Creating output directory: {OUTPUT_PATH}\")\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "print(f\"Output directory ensured at: {OUTPUT_PATH}\")\n",
        "\n",
        "# Load the full downloaded dataset\n",
        "full_dataset_path = os.path.join(DRIVE_PATH, INPUT_FILENAME)\n",
        "try:\n",
        "    print(f\"Loading full dataset from: {full_dataset_path}...\")\n",
        "    df_full = pd.read_csv(full_dataset_path)\n",
        "    print(f\"Successfully loaded dataset with {len(df_full)} rows.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: The file was not found at {full_dataset_path}. Please check the path and filename.\")\n",
        "    exit()\n",
        "\n",
        "# --- [CRITICAL UPDATE] ---\n",
        "# Create the instance_id column since it doesn't exist in the original file.\n",
        "# We will use the original row number (the index) as the unique ID.\n",
        "if INSTANCE_ID_COLUMN not in df_full.columns:\n",
        "    print(f\"'{INSTANCE_ID_COLUMN}' column not found. Creating it from the DataFrame's index...\")\n",
        "    df_full[INSTANCE_ID_COLUMN] = df_full.index\n",
        "    print(\"Successfully created instance_id column.\")\n",
        "else:\n",
        "    print(f\"'{INSTANCE_ID_COLUMN}' column already exists.\")\n",
        "\n",
        "# Verify that necessary columns exist AFTER creating the instance_id\n",
        "if INSTANCE_ID_COLUMN not in df_full.columns or TARGET_COLUMN not in df_full.columns:\n",
        "    print(f\"ERROR: The required columns ('{INSTANCE_ID_COLUMN}' or '{TARGET_COLUMN}') were not found in the CSV.\")\n",
        "    print(\"Please check the column names in your file and the script's configuration.\")\n",
        "    exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXj3bjsjli_Q",
        "outputId": "bc29b9a0-1d47-47ce-8e20-a7cea7c7787d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating output directory: /content/drive/My Drive/data_for_publication/\n",
            "Output directory ensured at: /content/drive/My Drive/data_for_publication/\n",
            "Loading full dataset from: /content/drive/My Drive/medal_training.csv...\n",
            "Successfully loaded dataset with 100000 rows.\n",
            "'instance_id' column not found. Creating it from the DataFrame's index...\n",
            "Successfully created instance_id column.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract ABBREV term from dataset and put in new column#\n",
        "import re\n",
        "def create_abbrev_and_extract_from_text(df):\n",
        "    \"\"\"\n",
        "    Creates an \"ABBREV\" column by extracting the abbreviation\n",
        "    from TEXT using the word in LOCATION index, stopping at a space.\n",
        "    \"\"\"\n",
        "\n",
        "    def extract_abbr(row):\n",
        "        text = row['TEXT']\n",
        "        location = row['LOCATION']\n",
        "\n",
        "        if isinstance(text, str):\n",
        "           try:\n",
        "             word_index = int(location)\n",
        "             words = re.findall(r'\\b\\w+\\b', text) # Tokenize by word\n",
        "             if 0 <= word_index < len(words):\n",
        "                extracted_abbrev = words[word_index] # Get word at index\n",
        "                return extracted_abbrev\n",
        "           except (ValueError, IndexError):\n",
        "               pass # Handle cases where location is invalid\n",
        "        return ''\n",
        "    df_full['ABBREV'] = df_full.apply(extract_abbr, axis=1)\n",
        "    print(\"Added 'ABBREV' column to DataFrame successfully.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "NEW_CSV_FILE_PATH = \"medal_with_abbrev.csv\"\n",
        "\n",
        "\n",
        "# Add new columns\n",
        "df_full = create_abbrev_and_extract_from_text(df_full)\n",
        "\n",
        "# Save the updated dataframe to a new csv file\n",
        "df_full.to_csv(NEW_CSV_FILE_PATH, index=False, encoding='utf-8')\n",
        "print(f\"Saved updated CSV to: {NEW_CSV_FILE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX4BECcLZlRt",
        "outputId": "6ec5a5bb-d33a-4e3e-8aa8-5f328edfc7b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 'ABBREV' column to DataFrame successfully.\n",
            "Saved updated CSV to: medal_with_abbrev.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract top frequencues Abbreviations#\n",
        "def extract_top_frequent_abbrevs(df, column_name, top_n):\n",
        "    \"\"\"\n",
        "    Extracts the top N most frequent values from a column in a DataFrame and stores them in a new DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        column_name (str): The name of the column to analyze.\n",
        "        top_n (int): The number of top frequent values to extract.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame with the top N most frequent values and their counts.\n",
        "    \"\"\"\n",
        "    # Count the occurrences of each value in the column\n",
        "    value_counts = df[column_name].value_counts()\n",
        "\n",
        "    # Extract the top N most frequent values\n",
        "    top_values = value_counts.nlargest(top_n)\n",
        "\n",
        "    # Convert the result to a DataFrame\n",
        "    top_df = pd.DataFrame({'ABBREV': top_values.index, 'count': top_values.values})\n",
        "    return top_df\n",
        "\n",
        "def filter_dataframe_by_top_abbrevs(df, column_name, top_abbreviations_df):\n",
        "    \"\"\"\n",
        "    Filters a DataFrame to keep rows where the specified column contains any of the top abbreviations.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        column_name (str): The name of the column containing the abbreviations.\n",
        "        top_abbreviations_df (pd.DataFrame): A DataFrame with the top abbreviations.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A filtered DataFrame containing only the rows with top abbreviations.\n",
        "    \"\"\"\n",
        "    top_abbrevs_list = top_abbreviations_df['ABBREV'].tolist()\n",
        "    filtered_df = df[df[column_name].isin(top_abbrevs_list)]\n",
        "    return filtered_df\n",
        "\n",
        "\n",
        "def save_dataframe_to_csv(df, output_file_path):\n",
        "    \"\"\"Saves the DataFrame to a CSV file.\"\"\"\n",
        "    df.to_csv(output_file_path, index=False)\n",
        "    print(f\"DataFrame saved to: {output_file_path}\")"
      ],
      "metadata": {
        "id": "YNa6qcrkbExC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "column_to_analyze = 'ABBREV'\n",
        "top_count = 500\n",
        "output_csv_file = 'MeDAL_Training_Subset.csv' # Specify the path to where you want to save\n",
        "\n",
        "# Get the top abbreviations\n",
        "top_abbreviations_df = extract_top_frequent_abbrevs(df_full, column_to_analyze, top_count)\n",
        "\n",
        "# Filter the dataframe\n",
        "filtered_df = filter_dataframe_by_top_abbrevs(df_full, column_to_analyze, top_abbreviations_df)\n",
        "\n",
        "# Save the filtered DataFrame to CSV\n",
        "save_dataframe_to_csv(filtered_df, output_csv_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC_1zM7RbIcb",
        "outputId": "250f73a1-6e61-4f5b-e9bb-33f2d95938ed"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame saved to: MeDAL_Training_Subset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 3. CREATE THE REPRODUCIBLE SUBSET ---\n",
        "N_SUBSET =filtered_df.shape[0] # Set the desired size of the subset\n",
        "print(f\"\\nCreating a reproducible subset of {N_SUBSET} instances...\")\n",
        "\n",
        "# We use .sample() with a fixed random_state. This guarantees that the\n",
        "# exact same \"random\" subset is chosen every time this script is run.\n",
        "df_subset = df_full.sample(n=N_SUBSET, random_state=RANDOM_SEED)\n",
        "df_subset = df_subset.reset_index(drop=True) # Reset index for clean processing\n",
        "\n",
        "print(f\"Subset created with {len(df_subset)} rows.\")\n",
        "\n",
        "# --- 4. CREATE STRATIFIED K-FOLD SPLITS ---\n",
        "\n",
        "print(f\"\\nGenerating {N_SPLITS}-fold stratified cross-validation splits...\")\n",
        "\n",
        "# Initialize StratifiedKFold. shuffle=True and a fixed random_state ensure\n",
        "# that the data is shuffled the same way before splitting every time.\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "# Add a new 'fold' column to our subset DataFrame to store the fold number\n",
        "df_subset['fold'] = -1\n",
        "\n",
        "# Get the data (X) and target (y) for splitting\n",
        "X = df_subset\n",
        "y = df_subset[TARGET_COLUMN]\n",
        "\n",
        "# Loop through the splits and assign each instance to a fold\n",
        "for fold_num, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    # The 'test_index' contains the indices for the instances in the current fold\n",
        "    df_subset.loc[test_index, 'fold'] = fold_num\n",
        "    print(f\"Assigned {len(test_index)} instances to fold {fold_num}.\")\n",
        "\n",
        "# Verify that all instances were assigned a fold\n",
        "if (df_subset['fold'] == -1).any():\n",
        "    print(\"WARNING: Some rows were not assigned to a fold. Please check the process.\")\n",
        "else:\n",
        "    print(\"All instances successfully assigned to a fold.\")\n",
        "\n",
        "# --- 5. SAVE THE OUTPUT FILES ---\n",
        "\n",
        "print(\"\\n--- Saving output files for publication ---\")\n",
        "\n",
        "# 1. The full subset data, including the new 'fold' column\n",
        "subset_filename = 'MeDAL_subset_with_folds.csv'\n",
        "subset_path = os.path.join(OUTPUT_PATH, subset_filename)\n",
        "df_subset.to_csv(subset_path, index=False)\n",
        "print(f\"1. Saved subset with fold information to: {subset_path}\")\n",
        "\n",
        "# 2. A simple text file with only the unique instance IDs of the subset\n",
        "ids_filename = 'MeDAL_subset_instance_ids.txt'\n",
        "ids_path = os.path.join(OUTPUT_PATH, ids_filename)\n",
        "df_subset[INSTANCE_ID_COLUMN].to_csv(ids_path, index=False, header=False)\n",
        "print(f\"2. Saved list of instance IDs to: {ids_path}\")\n",
        "\n",
        "# 3. A README file explaining the files\n",
        "readme_filename = 'README.txt'\n",
        "readme_path = os.path.join(OUTPUT_PATH, readme_filename)\n",
        "with open(readme_path, 'w') as f:\n",
        "    f.write(\"DATASET FILES FOR REPRODUCIBILITY\\n\")\n",
        "    f.write(\"=====================================\\n\\n\")\n",
        "    f.write(f\"This folder contains the data subset used in the paper: 'Harrensing Transformer Knowledge: A Novel Approach for Biomedical Abbreviation Disambiguation'.\\n\\n\")\n",
        "    f.write(f\"1. {subset_filename}:\\n\")\n",
        "    f.write(f\"   - Contains the full subset from the data with instances used in the study.\\n\")\n",
        "    f.write(\"   - The 'fold' column indicates which of the 10 cross-validation folds each instance belongs to (0-9).\\n\\n\")\n",
        "    f.write(f\"2. {ids_filename}:\\n\")\n",
        "    f.write(\"   - A lightweight text file containing only the unique instance IDs (corresponding to original row numbers) that make up our subset.\\n\")\n",
        "    f.write(\"   - This file can be used to reconstruct the exact subset from the original full dataset.\\n\")\n",
        "\n",
        "print(f\"3. Saved README file to: {readme_path}\")\n",
        "print(\"\\n--- Process complete! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zszYsytNZCwc",
        "outputId": "873e88c7-1b64-4350-c9e6-17a5865d2cb7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating a reproducible subset of 44669 instances...\n",
            "Subset created with 44669 rows.\n",
            "\n",
            "Generating 10-fold stratified cross-validation splits...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
             "--- Saving output files for publication ---\n",
           
            "--- Process complete! ---\n"
          ]
        }
      ]
    }
  ]
}
